## ğŸ¯ Project Overview

This project detects DeepFakes by analyzing:

* ğŸ¥ **Visual content:** Face manipulation detection
* ğŸ”Š **Audio content:** Voice synthesis detection
* ğŸ‘„ **Audioâ€“Visual Synchronization:** Lip-sync mismatch detection
* ğŸ”— **Multi-modal Fusion:** Combined decision-making

---

## ğŸ§© Classification Categories

| Label        | Category | Description                               |
| :----------- | :------- | :---------------------------------------- |
| **0 â€” Real** | (A, B)   | RealVideoâ€“RealAudio / RealVideoâ€“FakeAudio |
| **1 â€” Fake** | (C, D)   | FakeVideoâ€“RealAudio / FakeVideoâ€“FakeAudio |

> The model performs **binary classification** â€” determining whether a video is **authentic (Real)** or **manipulated (Fake)**.

---

## ğŸ“ Project Structure

```
deepfake_detection_2/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                         # Original FakeAVCeleb dataset
â”‚   â”‚   â”œâ”€â”€ RealVideo-RealAudio/     # Category A: Authentic
â”‚   â”‚   â”œâ”€â”€ RealVideo-FakeAudio/     # Category B: Voice cloned
â”‚   â”‚   â”œâ”€â”€ FakeVideo-RealAudio/     # Category C: Face swapped
â”‚   â”‚   â”œâ”€â”€ FakeVideo-FakeAudio/     # Category D: Both fake
â”‚   â”‚   â””â”€â”€ meta_data.csv            # Dataset metadata
â”‚   â””â”€â”€ processed/                   # Preprocessed features
â”‚       â”œâ”€â”€ frames/                  # Extracted video frames
â”‚       â”œâ”€â”€ faces/                   # Cropped face regions
â”‚       â”œâ”€â”€ audio/                   # Extracted audio files
â”‚       â”œâ”€â”€ audio_features/          # Mel-spectrograms
â”‚       â”œâ”€â”€ sync/                    # Lip-sync alignment data
â”‚       â””â”€â”€ fusion_features/         # Combined multi-modal features
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/               # Data preprocessing scripts
â”‚   â”œâ”€â”€ models/                      # Model architectures
â”‚   â”œâ”€â”€ training/                    # Training scripts
â”‚   â”œâ”€â”€ evaluation/                  # Evaluation and metrics
â”‚   â””â”€â”€ api/                         # (Future) FastAPI deployment
â”‚
â”œâ”€â”€ models/                          # Saved model weights
â”œâ”€â”€ results/                         # Evaluation results
â”œâ”€â”€ notebooks/                       # Jupyter notebooks for experiments
â””â”€â”€ requirements.txt                 # Python dependencies
```

---

## ğŸ”„ Complete Workflow

### 1ï¸âƒ£ **Data Preprocessing**

| Step                   | Script                                    | Description                                            | Output                           |
| ---------------------- | ----------------------------------------- | ------------------------------------------------------ | -------------------------------- |
| **Frame Extraction**   | `src/preprocessing/extract_frames.py`     | Extracts frames from videos                            | `data/processed/frames/`         |
| **Face Detection**     | `src/preprocessing/face_detection.py`     | Detects and crops faces                                | `data/processed/faces/`          |
| **Audio Extraction**   | `src/preprocessing/extract_audio.py`      | Extracts and converts audio to WAV                     | `data/processed/audio/`          |
| **Feature Extraction** | `src/preprocessing/feature_extraction.py` | Converts audio to mel-spectrograms; creates embeddings | `data/processed/audio_features/` |
| **Sync Data Creation** | `src/preprocessing/sync_preprocess.py`    | Aligns lips and audio for synchronization dataset      | `data/processed/sync/`           |

---

### 2ï¸âƒ£ **Model Training**

#### ğŸ§â€â™‚ï¸ Visual Model (`src/training/train_visual.py`)

* **Architecture:** ResNet-18 based CNN
* **Purpose:** Detects face manipulation artifacts
* **Input:** Face crops (224Ã—224)
* **Output:** Real / Fake classification
* **Saved at:** `models/visual_model/visual_model.pth`

---

#### ğŸ”Š Audio Model (`src/training/train_audio.py`)

* **Architecture:** CNN for spectrogram analysis
* **Purpose:** Detects voice cloning or synthetic speech
* **Input:** Mel-spectrograms (64Ã—160)
* **Output:** Real / Fake audio classification
* **Saved at:** `models/audio_model/audio_model.pth`

---

#### ğŸ‘„ Sync Model (`src/training/train_sync.py`)

* **Architecture:** CNN-based (similar to SyncNet)
* **Purpose:** Detects lipâ€“speech mismatches
* **Input:** Lip movement + audio alignment
* **Output:** Synced / Unsynced classification
* **Saved at:** `models/sync_model/sync_model.pth`

---

#### ğŸ”— Fusion Model (`src/training/train_fusion_transformer.py`)

* **Architecture:** Transformer-based encoder (tokenized fusion vector + positional encoding + TransformerEncoder)
* **Purpose:** Combines **visual**, **audio**, and **sync** features to make the final Real/Fake decision.
* **Input:** Concatenated embeddings â†’ split into token sequences for the Transformer.
* **Output:** **Binary classification (Real vs Fake)**

**Training Details:**

* **Loss:** CrossEntropyLoss
* **Optimizer:** Adam
* **LR Scheduler:** StepLR
* **Batch Size:** 16
* **Learning Rate:** 1e-4
* **Epochs:** 6
* **Saved Model:** `models/fusion_model/fusion_transformer.pth`

**Notes:**

* The fusion vector is padded and reshaped into `(seq_len, d_model)` tokens.
* Transformer learns cross-modal relationships through attention.

---

### 3ï¸âƒ£ **Model Evaluation**

| Component               | Script                             | Description                                                       | Output                    |
| ----------------------- | ---------------------------------- | ----------------------------------------------------------------- | ------------------------- |
| **Performance Metrics** | `src/evaluation/evaluate.py`       | Calculates accuracy, precision, recall, F1, ROC, confusion matrix | `results/evaluation/`     |
| **Explainability**      | `src/evaluation/explainability.py` | Grad-CAM & feature heatmaps                                       | `results/explainability/` |

---

### 4ï¸âƒ£ **API Deployment (Future Work)**

| File                                                                    | Description                                  |
| ----------------------------------------------------------------------- | -------------------------------------------- |
| `src/api/main.py`                                                       | FastAPI endpoint `/predict` for video upload |
| `src/api/inference.py`                                                  | Loads models, runs full inference pipeline   |
| **Planned Output:** JSON with Real/Fake prediction and confidence score |                                              |

**Example:**

```json
{
  "visual_pred": "Real",
  "audio_pred": "Fake",
  "fusion_pred": "Fake",
  "confidence": 0.97
}
```

---

## ğŸ“Š Dataset Information

**FakeAVCeleb Dataset**

* Generated using **Faceswap**, **FSGAN**, **Wav2Lip**, and **RTVC**.
* Each video is categorized by the authenticity of **video** and **audio** components.

| Technique | Description         |
| --------- | ------------------- |
| Faceswap  | Face replacement    |
| FSGAN     | Face reenactment    |
| Wav2Lip   | Lip synchronization |
| RTVC      | Voice cloning       |

---

## ğŸ“ Usage Example (After API Integration)

```python
import requests

with open("test_video.mp4", "rb") as f:
    response = requests.post("http://localhost:8000/predict", files={"file": f})

result = response.json()
print(f"Final Prediction: {result['fusion_pred']} ({result['confidence']:.2f})")
```

---

## ğŸ“ Research & Real-World Applications

* Social Media Content Verification
* News & Broadcast Authenticity Checking
* Digital Forensics & Law Enforcement
* Media Literacy & Awareness
* Academic Research on DeepFake Detection

---

## ğŸš€ Future Enhancements

* Real-time video stream analysis
* FastAPI deployment (inference endpoint)
* Mobile and cloud deployment support
* Integration of new deepfake generation types
* Cross-modal contrastive learning (future research)
